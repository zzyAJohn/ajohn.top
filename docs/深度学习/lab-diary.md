---
title: 科研碎碎念
createTime: 2025/01/04 16:09:40
permalink: /article/4svrfjec/
sticky: 1
---

虽说“[周记还是别人写的好看](https://yoyoyoake.github.io/article/xaklzim6/)”，但总归也是要稍微记录一下的，好记性不如烂键盘嘛。嗯...就从科研开始吧，按照倒序来排版。
<!-- more -->

## 2025.1.10

有朋自远方来，遂游玩一天

晚上改了模型一阶段的loss，尝试跑一下


## 2025.1.9

放弃act方法了，转战tsnt了

下阶段任务：
- 复现tsnt
- 画re-id方法图
- 1-2-1-2阶段交错
- 调研NLL文献
- 给lcnl写个issue done




## 2025.1.8

现在开始测试两个网络分别赋予不同的学习率

---

今早昨天的实验跑完了，整理如下：

测试了四组学习率
- 这是core的精度：
```bash
Epoch:[60][400/405] Time: 0.168 (0.174) Loss1: 0.5615 (0.6350) Loss2: 0.6325 (0.6285) Acc1: 49.35  Acc2: 49.35
>> Epoch: 60 | R1: 87.7375% | R5: 95.4276% | R10: 96.7933% | mAP: 63.8193% (Best Epoch[46]) | Evaluation time: 57.03s
```
以下是测试的：
### 小学习率：
- 0.001
```bash
Epoch:[60][400/405] Time: 0.171 (0.174) Loss1: 2.5511 (2.4859) Loss2: 3.4698 (3.4731) Acc1: 89.95  Acc2: 96.49
>> Epoch: 60 | R1: 79.0677% | R5: 92.5178% | R10: 95.1603% | mAP: 41.3135% (Best Epoch[6]) | Evaluation time: 54.14s
```
map没涨


### 正常学习率：
- 0.01
```bash
Epoch:[60][400/405] Time: 0.175 (0.176) Loss1: 2.4211 (2.4028) Loss2: 3.3596 (3.3543) Acc1: 69.58  Acc2: 98.83
>> Epoch: 60 | R1: 86.1936% | R5: 95.3682% | R10: 97.0012% | mAP: 59.2124% (Best Epoch[50]) | Evaluation time: 57.06s
```
跑不过，但差的也不多，准确率都比core高
损失为什么降不下去？

### 大学习率：
- 0.05
```bash
Epoch:[60][400/405] Time: 0.170 (0.175) Loss1: 2.1662 (nan) Loss2: 3.9607 (4.0145) Acc1: 18.05  Acc2: 71.35
>> Epoch: 60 | R1: 64.2518% | R5: 81.0273% | R10: 86.1639% | mAP: 29.6619% (Best Epoch[58]) | Evaluation time: 53.51s
```
结论：如果在第一阶段结束的时候map较低，那么二阶段损失会出现nan

- 0.1
```bash
Epoch:[0][400/405] Time: 0.165 (0.169) Loss1: 536340064.0000 (37130636.9201) Loss2: 6.6077 (6.7392) Acc1: 0.24  Acc2: 0.33
>> Epoch: 60 | R1: 0.1485% | R5: 0.1485% | R10: 0.1485% | mAP: 0.5422% (Best Epoch[0]) | Evaluation time: 51.20s
```
损失函数直接就飞出去了，训练不起来，一开始不能用大学习率


## 2025.1.7



### act
- indices是什么？应该是所有样本的唯一索引
```py
print('indices是什么？')
print(len(indices))
print(len(indices[0]))
print(indices)
```
```bash
indices是什么？
32
tensor([ 2579,  3457, 10886,  9216,  1809, 10596, 10518,  6029, 10882,  8314,
         7502, 12229,  8194,  4152,  6563, 10842,  9713,  7919,  4667,  3683,
        10413,  1568,  5601,  7377,   875,  7728,  7984,  3121,  5691,   963,
         1772,  4685], device='cuda:0')
```
- logits0是什么？对应core中的output1
```py
print('logits0是什么？')
print(len(logits0)) # 长度为batchsize，32
print(len(logits0[0])) # 每个列表有751个列，即训练集类别数
print(logits0)
```
```bash
logits0是什么？
32
751
tensor([[ 0.5732,  0.0670, -0.3982,  ...,  0.0276, -0.3396, -0.0323],
        [ 0.3193, -0.0851, -0.2014,  ...,  0.2150,  0.1199, -0.0277],
        [ 0.3801, -0.0467, -0.3347,  ...,  0.3401,  0.1827, -0.1226],
        ...,
        [ 0.3848, -0.0105, -0.2947,  ...,  0.1929, -0.0128, -0.0216],
        [ 0.3167, -0.0809, -0.1807,  ...,  0.2251,  0.1580, -0.0376],
        [ 0.4285, -0.0119, -0.2646,  ...,  0.1434, -0.0654, -0.0278]],
       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
```
- prediction0是什么？
```py
print(prediction0)
```
```bash
tensor([412,  87, 182,  77, 461, 331, 502, 696, 373, 684, 696,   9, 713,  92,
         86,  92, 491, 461, 461, 480, 677, 208, 502, 529, 662, 182, 662,  92,
        122, 461, 677, 461] # 每个样本的预测类别
```

- y 是什么？
```bash
tensor([249, 719, 486,  46,  99, 367, 433, 695, 136,  25, 580, 246, 243, 401,
        236, 516, 211, 745, 227, 636,  98, 657,  91, 544,  78, 689, 628, 299,
        576, 369,  61, 110], device='cuda:0')
```

- weight的长度是？
```py
12936 # 每个样本有一个自己的权重
```
- 权重weights是什么？每个样本有一个自己的权重
```py
print(weights)
print(len(weights))
```
```bash
tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0', dtype=torch.int32)
12936
```
### core

- weight_r 指的是 损失函数里面标签的损失的权重，当损失减少时，预测损失权重变大
```
# 计算加权损失
# 当 λ = 0 时，损失为标准交叉熵损失。
# 当 λ = 1 时，损失为最大预测标签的损失。
loss = - (1 - lambda1) * logpt_gt - lambda1 * logpt_pred
```

- 这是什么？
```py
print('这是什么？')
print(len(trainLabels_nsy))
```
12936

- index是什么？
```bash
tensor([ 9027, 11786,   938, 12064,  4273,  9431,  3019, 11065,  7725, 11628,
         1221, 11186,  5034,  5410,  9266,  8423,  6885,   131,  4721,  6996,
         2788,  7949, 11218,  7209,  3846,  5938,  3747,  1326,  6675,  8388,
         8812, 12734])
```
- weights是什么？
```bash
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:2')
```
- targets是什么？
```bash
tensor([ 33, 573, 681, 588,  49, 474, 121, 279, 497, 248,  27, 383, 387, 152,
        731, 607, 444, 329, 461, 121, 581, 408, 750, 369,  66,  54, 252,   3,
        338, 492, 343, 533], device='cuda:3')
```
- outputs1是什么？
```py 
print(len(outputs1))
print(len(outputs1[0]))
print(outputs1)
```
```bash
32
751
tensor([[ 0.3569, -0.5325,  0.2461,  ...,  0.2276,  0.8800,  0.9543],
        [ 0.7879, -0.9050, -0.1885,  ...,  0.8721,  2.0121,  2.8518],
        [ 0.8961, -0.6164,  0.2098,  ...,  0.2175,  0.1401,  0.6251],
        ...,
        [ 1.2746, -0.6382,  0.1917,  ...,  0.1243,  0.9910,  1.0498],
        [ 0.6368, -0.6857,  0.1564,  ...,  0.2204, -0.7413,  1.1805],
        [-0.1724, -0.6851,  0.0175,  ...,  0.2778,  0.1517,  3.4362]],
       device='cuda:3', grad_fn=<AddmmBackward0>)
```
## 2025.1.6

难点：怎么把两个干净的网络改成一个干净一个噪声呢？


## 2025.1.5

- 尝试在core中加入minp

结果：失败，cython看不懂，python作者给的源码就报错，暂时放弃

doing：正在研究core和act的weight

初步结论：core没用过，所有样本一直是1；act在预热每轮+1

鲁棒训练日志中有：
```bash
选择了什么？
tensor([3], device='cuda:0', dtype=torch.int32)
ROBUST TRAINING:  61%|> | 248/405 [00:47<00:32,  4.80it/s, TrainAcc_net_0: 0.58%; TrainAcc_nROBUST TRAINING:  61%|> | 249/405 [00:47<00:31,  5.01it/s, TrainAcc_net_0: 0.58%; TrainAcc_net_1: 97.10%; TrainLoss_net_0: nan; TrainLoss_net_1: 3.30]-----------权重是什么-------
tensor([2, 4], device='cuda:0', dtype=torch.int32)
选择了什么？
tensor([3, 5], device='cuda:0', dtype=torch.int32)
```
说明act在鲁棒训练的时候只选择特定的样本进行加权

## 2025.1.4

- 测试不同gpu性能差距：有差距！
- 测试相同gpu不同时间有无差距：有差距！

结论：那我就随便跑了，反正结果有误差

- 复现self结果

结果：done，还不错
