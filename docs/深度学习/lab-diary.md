---
title: 科研碎碎念
createTime: 2025/01/04 16:09:40
permalink: /article/4svrfjec/
draft: true
---

虽说“[周记还是别人写的好看](https://yoyoyoake.github.io/article/xaklzim6/)”，但总归也是要稍微记录一下的，好记性不如烂键盘嘛。嗯...就从科研开始吧。
<!-- more -->
## 2025.1.23 测试样本重加权
今早一来发现不同噪声率的性能怎么一样？发现忘记生成噪声标签了，我还以为最近有提升呢，前功尽弃

没关系，重新来过

### 基线方法：core_0
0. 一阶段20周期：标签修复，（1- weight_r）* 交叉熵（输出1和标签）+ weight_r *（输出1和预测1）58.14
   二阶段40周期：协同训练，交叉熵（输出1和预测2） + kl散度67.36
共计60周期

以下是我的尝试：

### 方法：core_1 纯样本
1. 一阶段20周期：标签修复，（1- weight_r）* 交叉熵（输出1和标签）+ weight_r *（输出1和预测1）+ ①纯样本(预测1=标签)60.56
   二阶段40周期：协同训练，交叉熵（输出1和预测2） + kl散度68.14
共计60周期

### 方法：core_1_1 纯样本 考虑可能前期会蒙对，降低权重
1_1 一阶段20周期：标签修复，（1- weight_r）* 交叉熵（输出1和标签）+ weight_r *（输出1和预测1）+ ①weight_r *纯样本(预测1=标签)58.74
   二阶段40周期：协同训练，交叉熵（输出1和预测2） + kl散度67.46
共计60周期

### 方法：core_2 硬样本重加权
2. 一阶段20周期：标签修复，（1- weight_r）* 交叉熵（输出1和标签）+ weight_r *（输出1和预测1）56.64
   二阶段40周期：协同训练，交叉熵（输出1和预测2） + kl散度 + ②硬样本重加权（输出1和新标签）（新标签是模型1的预测）64.77
共计60周期
PS：看一下两个网络哪个结果好

### 方法：core_2_1 硬样本重加权
2_1. 一阶段20周期：标签修复，（1- weight_r）* 交叉熵（输出1和标签）+ weight_r *（输出1和预测1）57.29
   二阶段40周期：协同训练，交叉熵（输出1和预测2） + kl散度66.43
   三阶段60周期：标签修复，②硬样本重加权（输出1和新标签）（新标签是模型自己的预测）65.98
   四阶段80周期：协同训练，交叉熵（输出1和预测2） + kl散度  66.59

共计80周期

### 方法：core_2_2 硬样本重加权
2_1. 一阶段20周期：标签修复，（1- weight_r）* 交叉熵（输出1和标签）+ weight_r *（输出1和预测1）+ ①weight_r *纯样本(预测1=标签)60.81
   二阶段40周期：协同训练，交叉熵（输出1和预测2） + kl散度67.57
   三阶段60周期：标签修复，0.2* 交叉熵（输出1和标签）+ 0.8 *（输出1和预测1）+  ①纯样本(预测1=标签且概率大于0.9)66.08
   四阶段80周期：协同训练，交叉熵（输出1和预测2） + kl散度 67.65

共计80周期

### 方法：core_3 lre损失
3. 一阶段20周期：标签修复，（1- weight_r）* 交叉熵（输出1和标签）+ weight_r *（输出1和预测1）57.17
   二阶段40周期：协同训练，交叉熵（输出1和预测2） + kl散度 + ③lre损失（tsnt可靠样本）67.15
共计60周期

### 方法：core_3_1 lre损失
3_1 一阶段20周期：标签修复，（1- weight_r）* 交叉熵（输出1和标签）+ weight_r *（输出1和预测1）57.46
   二阶段40周期：协同训练，交叉熵（输出1和预测2） + kl散度 + ③lre损失（预测1=预测2=标签）66.95
共计60周期

### 方法：core_3_2 lre损失
3_2 一阶段20周期：标签修复，（1- weight_r）* 交叉熵（输出1和标签）+ weight_r *（输出1和预测1）
   二阶段40周期：协同训练，交叉熵（输出1和预测2） + kl散度 + ③lre损失（预测1=预测2=标签且p大于0.95）
共计60周期

### 方法：tsnt_0 tsnt
3_2 一阶段20周期：标签修复，0.8* 交叉熵（输出1和标签）+ 0.2 *（输出1和预测1）+ 纯样本(预测1=标签)59.44
   二阶段40周期：协同训练，kl散度 + ③lre损失（交叉熵）67.37
共计60周期

### 方法：tsnt_1 tsnt使用原始三元组损失
3_2 一阶段20周期：标签修复，0.8* 交叉熵（输出1和标签）+ 0.2 *（输出1和预测1）+ 纯样本(预测1=标签)60.04
   二阶段40周期：协同训练，kl散度 + ③lre损失（交叉熵）+原始三元组损失66.62
共计60周期

### 方法：tsnt_2 tsnt使用lcnl三元组损失 doing
3_2 一阶段20周期：标签修复，0.8* 交叉熵（输出1和标签）+ 0.2 *（输出1和预测1）+ 纯样本(预测1=标签)
   二阶段40周期：协同训练，kl散度 + ③lre损失（交叉熵）+原始三元组损失
共计60周期



## 2025.1.22 读文献寻找灵感

测试一下在duke数据集上的性能

## 2025.1.21 测试样本重加权

好像有效果，map到71%了，rank都接近sota了！

## 2025.1.20 测试样本重加权

1. 一阶段：标签修复，0.8交叉熵（输出1和标签）+ 0.2（输出1和预测1）+（纯样本）
   二阶段：协同训练，交叉熵（输出1和预测2） + kl散度 + 样本重加权（输出1和标签）
71.2

2. 一阶段：标签修复，0.8交叉熵（输出1和标签）+ 0.2（输出1和预测1）+（纯样本）
   二阶段：协同训练，交叉熵（输出1和预测2） + kl散度 + 样本重加权（输出1和预测2）
71.9

3. 一阶段：标签修复，加权交叉熵（输出1和标签）+ 加权（输出1和预测1）+（纯样本）
   二阶段：协同训练，交叉熵（输出1和预测2） + kl散度 + 样本重加权（输出1和预测2）
71.4

4. 一阶段：标签修复，加权交叉熵（输出1和标签）+ 加权（输出1和预测1）+（纯样本）
   二阶段：协同训练，交叉熵（输出1和预测2） + kl散度 + 样本重加权（输出1和预测2） + 40阶段再次重加权
共计60周期
72.1

5. 一阶段：标签修复，加权交叉熵（输出1和标签）+ 加权（输出1和预测1）+（纯样本）
   二阶段：协同训练，交叉熵（输出1和预测2） + kl散度 + 样本重加权（输出1和预测2） + 40阶段再次重加权 + 60阶段再次重加权
共计80周期
71.7

6. 一阶段：标签修复，加权交叉熵（输出1和标签）+ 加权（输出1和预测1）+2*（纯样本）
   二阶段：协同训练，交叉熵（输出1和预测2） + kl散度 + 样本重加权（输出1和预测2） + 40阶段再次重加权 + 60阶段再次重加权
共计80周期
71.1

7. 一阶段20周期：标签修复，加权交叉熵（输出1和标签）+ 加权（输出1和预测1）+（纯样本）
   二阶段40周期：协同训练，交叉熵（输出1和预测2） + kl散度 + 20周期样本重加权（输出1和预测2）+lre损失（可靠样本）
共计60周期
PS：主要看看可靠样本的掩码前后有没有区分度，尝试更换可靠样本的选择
71.5

8. 一阶段20周期：标签修复，加权交叉熵（输出1和标签）+ 加权（输出1和预测1）+（纯样本）
   二阶段40周期：协同训练，交叉熵（输出1和预测2） + kl散度 + 20周期样本重加权（输出1和预测2）+lre损失（预测1=预测2=标签）
共计60周期

## 2025.1.19 去花博汇拍花，摆

## 2025.1.18 学调色，摆

## 2025.1.17 学调色，摆

## 2025.1.16 去东湖和省博玩，摆

## 2025.1.15 lcnl方法的正负样本损失

加入lcnl方法的正负样本损失，意料之内没有提升，不嘻嘻。


## 2025.1.14 实现ltr损失

用 gpt 实现了 ltr 损失，一通大改耗时四五天只提了 2 个百分点还是 11 号提升的，遂摆。

## 2025.1.13 实现lre损失

初步实现了二阶段的lre损失，好像没提升，摆。


## 2025.1.12 研究变量
```
output1是什么
torch.Size([32, 751])
tensor([[-0.0056, -0.0375, -0.0458,  ..., -0.0370, -0.0333, -0.0165],
        [-0.1013, -0.0383, -0.0506,  ..., -0.0388, -0.0650, -0.0478],
        [-0.0460, -0.0835, -0.0314,  ..., -0.0199, -0.0391, -0.0354],
        ...,
        [-0.0499, -0.0621, -0.0541,  ..., -0.0779, -0.0437, -0.0374],
        [-0.0184, -0.0496, -0.0173,  ..., -0.0505, -0.0279, -0.0295],
        [-0.0282, -0.0313, -0.0310,  ..., -0.0560, -0.0552, -0.0184]],
       device='cuda:0', grad_fn=<AddmmBackward0>)
```
```
output1_log_prob是什么
torch.Size([32, 751])
tensor([[-7.9087, -7.9405, -7.9488,  ..., -7.9400, -7.9363, -7.9196],
        [-9.2627, -9.1996, -9.2120,  ..., -9.2001, -9.2263, -9.2092],
        [-7.6884, -7.7259, -7.6738,  ..., -7.6623, -7.6815, -7.6778],
        ...,
        [-9.2391, -9.2512, -9.2432,  ..., -9.2670, -9.2328, -9.2265],
        [-7.6764, -7.7076, -7.6752,  ..., -7.7085, -7.6859, -7.6874],
        [-9.4243, -9.4274, -9.4271,  ..., -9.4521, -9.4512, -9.4145]],
       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)
```
```
output2_prob是什么
torch.Size([32, 751])
tensor([[0.0012, 0.0012, 0.0013,  ..., 0.0012, 0.0012, 0.0012],
        [0.0011, 0.0012, 0.0011,  ..., 0.0011, 0.0011, 0.0012],
        [0.0011, 0.0011, 0.0011,  ..., 0.0011, 0.0011, 0.0012],
        ...,
        [0.0011, 0.0011, 0.0011,  ..., 0.0011, 0.0012, 0.0011],
        [0.0011, 0.0011, 0.0011,  ..., 0.0011, 0.0011, 0.0011],
        [0.0012, 0.0012, 0.0012,  ..., 0.0012, 0.0013, 0.0012]],
       device='cuda:0')
```
```
pred_label是什么
torch.Size([32])
tensor([725, 476, 476, 476, 476, 476, 476, 476, 476, 476, 476, 476, 476, 476,
        476, 725, 476, 476, 476, 476, 476, 476, 725, 476, 725, 725, 476, 476,
        725, 476, 476, 476], device='cuda:0')
```
```
base_loss是什么
torch.Size([])
tensor(20.3855, device='cuda:0', grad_fn=<NllLossBackward0>)
```
## 2025.1.11 mask掩码好像有用

今早验收实验，发现提升了一点点

tsnt：
```bash
>> Epoch: 60 | R1: 90.2910% | R5: 97.1793% | R10: 98.0701% | mAP: 69.3951% (Best Epoch[42]) | Evaluation time: 62.61s
```
core：
```bash
>> Epoch: 60 | R1: 89.9644% | R5: 96.7933% | R10: 98.2185% | mAP: 68.2213% (Best Epoch[58]) | Evaluation time: 53.51s
```

```py
# print('predicted:')
# print(predicted)
# print('target:')
# print(target)
```
predicted:
tensor([398, 193,  59, 503, 211,  59,  59, 166,  59,  59,  59,  59, 398, 193,
        193, 398,  59,  59, 211, 398,  59, 398,  59, 503, 211,  59,  59,  59,
         59,  59, 398, 134], device='cuda:2')
target:
tensor([463, 132,  65, 403, 353, 375, 299, 452, 628, 165,  80, 474, 195, 375,
        577, 599,  27, 244, 422, 466, 562, 507, 479, 490, 581, 137, 718, 365,
        694, 740, 387, 608], device='cuda:2')

## 2025.1.10 加入mask掩码

有朋自远方来，遂游玩一天

晚上改了模型一阶段的loss，加入mask掩码，关注纯样本，尝试跑一下
- logpt是什么？是32x751的张量
```py
# 计算输入 logits 的 log-softmax
logpt = F.log_softmax(input, dim=1)
# print('logpt形状是？')
# print(logpt.shape) # torch.Size([32, 751])
```

- logpt_gt是什么？最后是一个每个批量样本最大可能类别
```py
# 获取目标标签对应的 log-probability
logpt_gt = logpt.gather(1, target.unsqueeze(1))  # gather 取得对应目标标签的 log-probability
# print('logpt_gt形状是？')
# print(logpt_gt.shape) # torch.Size([32, 1])
logpt_gt = logpt_gt.view(-1)  # 转换为一维张量
# print('logpt_gt形状是？')
# print(logpt_gt.shape) # torch.Size([32])，一个列表
```

- logpt_pred是什么？和logpt_gt类似，不过是预测损失
```py
# 获取模型预测的最大 log-probability
logpt_pred, predicted = torch.max(logpt, 1)
mask = (predicted == target).float()  # mask 的形状为 [batch_size]
print('mask:')
print(mask)
# 计算一致性损失 L_m
Lm = mask * logpt_gt  # 只考虑预测正确的样本的 log-probabilities
# print('logpt_pred形状是？')
# print(logpt_pred.shape) # torch.Size([32])，一个列表
```


## 2025.1.9 放弃act

放弃act方法了，转战tsnt了

下阶段任务：
- 复现tsnt
- 画re-id方法图
- 1-2-1-2阶段交错
- 调研NLL文献
- 给lcnl写个issue done




## 2025.1.8 测试不同 lr

现在开始测试两个网络分别赋予不同的学习率

结果：均G，不管怎么改，最后都超不过core，倒是能超过self

结论：只能说act有用但不多

---

今早昨天的实验跑完了，整理如下：

测试了四组学习率
- 这是core的精度：
```bash
Epoch:[60][400/405] Time: 0.168 (0.174) Loss1: 0.5615 (0.6350) Loss2: 0.6325 (0.6285) Acc1: 49.35  Acc2: 49.35
>> Epoch: 60 | R1: 87.7375% | R5: 95.4276% | R10: 96.7933% | mAP: 63.8193% (Best Epoch[46]) | Evaluation time: 57.03s
```
以下是测试的：
### 小学习率：
- 0.001
```bash
Epoch:[60][400/405] Time: 0.171 (0.174) Loss1: 2.5511 (2.4859) Loss2: 3.4698 (3.4731) Acc1: 89.95  Acc2: 96.49
>> Epoch: 60 | R1: 79.0677% | R5: 92.5178% | R10: 95.1603% | mAP: 41.3135% (Best Epoch[6]) | Evaluation time: 54.14s
```
map没涨


### 正常学习率：
- 0.01
```bash
Epoch:[60][400/405] Time: 0.175 (0.176) Loss1: 2.4211 (2.4028) Loss2: 3.3596 (3.3543) Acc1: 69.58  Acc2: 98.83
>> Epoch: 60 | R1: 86.1936% | R5: 95.3682% | R10: 97.0012% | mAP: 59.2124% (Best Epoch[50]) | Evaluation time: 57.06s
```
结果：跑不过，但差的也不多，准确率都比core高

疑惑：损失为什么降不下去？

### 大学习率：
- 0.05
```bash
Epoch:[60][400/405] Time: 0.170 (0.175) Loss1: 2.1662 (nan) Loss2: 3.9607 (4.0145) Acc1: 18.05  Acc2: 71.35
>> Epoch: 60 | R1: 64.2518% | R5: 81.0273% | R10: 86.1639% | mAP: 29.6619% (Best Epoch[58]) | Evaluation time: 53.51s
```
结论：如果在第一阶段结束的时候map较低，那么二阶段损失会出现nan

- 0.1
```bash
Epoch:[0][400/405] Time: 0.165 (0.169) Loss1: 536340064.0000 (37130636.9201) Loss2: 6.6077 (6.7392) Acc1: 0.24  Acc2: 0.33
>> Epoch: 60 | R1: 0.1485% | R5: 0.1485% | R10: 0.1485% | mAP: 0.5422% (Best Epoch[0]) | Evaluation time: 51.20s
```
结果：损失函数直接就飞出去了，训练不起来

结论：一开始不能用大学习率

## 2025.1.7 研究变量



### act
- indices是什么？应该是所有样本的唯一索引
```py
print('indices是什么？')
print(len(indices))
print(len(indices[0]))
print(indices)
```
```bash
indices是什么？
32
tensor([ 2579,  3457, 10886,  9216,  1809, 10596, 10518,  6029, 10882,  8314,
         7502, 12229,  8194,  4152,  6563, 10842,  9713,  7919,  4667,  3683,
        10413,  1568,  5601,  7377,   875,  7728,  7984,  3121,  5691,   963,
         1772,  4685], device='cuda:0')
```
- logits0是什么？对应core中的output1
```py
print('logits0是什么？')
print(len(logits0)) # 长度为batchsize，32
print(len(logits0[0])) # 每个列表有751个列，即训练集类别数
print(logits0)
```
```bash
logits0是什么？
32
751
tensor([[ 0.5732,  0.0670, -0.3982,  ...,  0.0276, -0.3396, -0.0323],
        [ 0.3193, -0.0851, -0.2014,  ...,  0.2150,  0.1199, -0.0277],
        [ 0.3801, -0.0467, -0.3347,  ...,  0.3401,  0.1827, -0.1226],
        ...,
        [ 0.3848, -0.0105, -0.2947,  ...,  0.1929, -0.0128, -0.0216],
        [ 0.3167, -0.0809, -0.1807,  ...,  0.2251,  0.1580, -0.0376],
        [ 0.4285, -0.0119, -0.2646,  ...,  0.1434, -0.0654, -0.0278]],
       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
```
- prediction0是什么？
```py
print(prediction0)
```
```bash
tensor([412,  87, 182,  77, 461, 331, 502, 696, 373, 684, 696,   9, 713,  92,
         86,  92, 491, 461, 461, 480, 677, 208, 502, 529, 662, 182, 662,  92,
        122, 461, 677, 461] # 每个样本的预测类别
```

- y 是什么？
```bash
tensor([249, 719, 486,  46,  99, 367, 433, 695, 136,  25, 580, 246, 243, 401,
        236, 516, 211, 745, 227, 636,  98, 657,  91, 544,  78, 689, 628, 299,
        576, 369,  61, 110], device='cuda:0')
```

- weight的长度是？
```py
12936 # 每个样本有一个自己的权重
```
- 权重weights是什么？每个样本有一个自己的权重
```py
print(weights)
print(len(weights))
```
```bash
tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0', dtype=torch.int32)
12936
```
### core

- weight_r 指的是 损失函数里面标签的损失的权重，当损失减少时，预测损失权重变大
```
# 计算加权损失
# 当 λ = 0 时，损失为标准交叉熵损失。
# 当 λ = 1 时，损失为最大预测标签的损失。
loss = - (1 - lambda1) * logpt_gt - lambda1 * logpt_pred
```

- 这是什么？
```py
print('这是什么？')
print(len(trainLabels_nsy))
```
12936

- index是什么？
```bash
tensor([ 9027, 11786,   938, 12064,  4273,  9431,  3019, 11065,  7725, 11628,
         1221, 11186,  5034,  5410,  9266,  8423,  6885,   131,  4721,  6996,
         2788,  7949, 11218,  7209,  3846,  5938,  3747,  1326,  6675,  8388,
         8812, 12734])
```
- weights是什么？
```bash
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:2')
```
- targets是什么？
```bash
tensor([ 33, 573, 681, 588,  49, 474, 121, 279, 497, 248,  27, 383, 387, 152,
        731, 607, 444, 329, 461, 121, 581, 408, 750, 369,  66,  54, 252,   3,
        338, 492, 343, 533], device='cuda:3')
```
- outputs1是什么？
```py 
print(len(outputs1))
print(len(outputs1[0]))
print(outputs1)
```
```bash
32
751
tensor([[ 0.3569, -0.5325,  0.2461,  ...,  0.2276,  0.8800,  0.9543],
        [ 0.7879, -0.9050, -0.1885,  ...,  0.8721,  2.0121,  2.8518],
        [ 0.8961, -0.6164,  0.2098,  ...,  0.2175,  0.1401,  0.6251],
        ...,
        [ 1.2746, -0.6382,  0.1917,  ...,  0.1243,  0.9910,  1.0498],
        [ 0.6368, -0.6857,  0.1564,  ...,  0.2204, -0.7413,  1.1805],
        [-0.1724, -0.6851,  0.0175,  ...,  0.2778,  0.1517,  3.4362]],
       device='cuda:3', grad_fn=<AddmmBackward0>)
```
## 2025.1.6 看文献

难点：怎么把两个干净的网络改成一个干净一个噪声呢？


## 2025.1.5 改代码

- 尝试在core中加入minp

结果：失败，cython看不懂，python作者给的源码就报错，暂时放弃

doing：正在研究core和act的weight

初步结论：core没用过，所有样本一直是1；act在预热每轮+1

鲁棒训练日志中有：
```bash
选择了什么？
tensor([3], device='cuda:0', dtype=torch.int32)
ROBUST TRAINING:  61%|> | 248/405 [00:47<00:32,  4.80it/s, TrainAcc_net_0: 0.58%; TrainAcc_nROBUST TRAINING:  61%|> | 249/405 [00:47<00:31,  5.01it/s, TrainAcc_net_0: 0.58%; TrainAcc_net_1: 97.10%; TrainLoss_net_0: nan; TrainLoss_net_1: 3.30]-----------权重是什么-------
tensor([2, 4], device='cuda:0', dtype=torch.int32)
选择了什么？
tensor([3, 5], device='cuda:0', dtype=torch.int32)
```
说明act在鲁棒训练的时候只选择特定的样本进行加权

## 2025.1.4 复现实验

- 测试不同gpu性能差距：有差距！
- 测试相同gpu不同时间有无差距：有差距！

结论：那我就随便跑了，反正结果有误差

- 复现self结果

结果：done，还不错
